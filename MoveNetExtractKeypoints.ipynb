{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toCy3v03Dwx7"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QKe-ubNcDvgv"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqtQzBCpIJ7Y"
   },
   "source": [
    "# MoveNet: Ultra fast and accurate pose detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u_VGR6_BmbZ"
   },
   "source": [
    "## Visualization libraries & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtcwSIcgbIVN"
   },
   "source": [
    "!pip install -q imageio\n",
    "!pip install -q opencv-python\n",
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9BLeJv-pCCld"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:19:00.668783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q imageio\n",
    "# !pip install -q opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "bEJBMeRb3YUy"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, fps):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, fps=fps)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvrN0iQiOxhR"
   },
   "source": [
    "## Load Model from TF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:19:10.101610: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-29 00:19:11.125530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:11.126413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.515GHz coreCount: 14 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2021-10-29 00:19:11.126452: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-29 00:19:11.129566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-29 00:19:11.132696: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-29 00:19:11.133345: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-29 00:19:11.136492: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-29 00:19:11.138395: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-29 00:19:11.145037: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-29 00:19:11.145434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:11.146398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:11.147124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if sys.platform == 'linux':\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if len(physical_devices) > 0:\n",
    "     tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zeGHgANcT7a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:19:16.142644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-29 00:19:16.172987: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2295730000 Hz\n",
      "2021-10-29 00:19:16.173799: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25cbc20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-10-29 00:19:16.173842: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-10-29 00:19:16.291460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:16.292440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3fbb730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-10-29 00:19:16.292477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2021-10-29 00:19:16.292882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:16.293664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.515GHz coreCount: 14 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2021-10-29 00:19:16.293718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-29 00:19:16.293770: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-29 00:19:16.293795: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-29 00:19:16.293819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-29 00:19:16.293842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-29 00:19:16.293865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-29 00:19:16.293888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-29 00:19:16.294028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:16.294872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:16.295592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-10-29 00:19:16.295648: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-29 00:19:17.273937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-29 00:19:17.274014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2021-10-29 00:19:17.274029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2021-10-29 00:19:17.274537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:17.275458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 00:19:17.276228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3408 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \n",
    "                    # \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoint_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoint_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKm-B0eMYeg8"
   },
   "source": [
    "## Video keypoints extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdPFXabLyiKv"
   },
   "source": [
    "This section demonstrates how to apply intelligent cropping based on detections from the previous frame when the input is a sequence of frames. This allows the model to devote its attention and resources to the main subject, resulting in much better prediction quality without sacrificing the speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "SYFdK-JHYhrv"
   },
   "outputs": [],
   "source": [
    "#@title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inferece on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2JmA1xAEntQ"
   },
   "source": [
    "### Load videos, extract keypoints and save to *.npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start id0_jab_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 00:19:58.085940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-29 00:20:00.220904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done id0_jab_1, 951 frames\n",
      "Start id0_jab_2\n",
      "Done id0_jab_2, 992 frames\n",
      "Start id0_hook_1\n",
      "Done id0_hook_1, 1099 frames\n",
      "Start id0_hook_2\n",
      "Done id0_hook_2, 1050 frames\n",
      "Start id0_uper_1\n",
      "Done id0_uper_1, 1199 frames\n",
      "Start id0_uper_2\n",
      "Done id0_uper_2, 1185 frames\n",
      "Start id1_jab_1\n",
      "Done id1_jab_1, 801 frames\n",
      "Start id1_jab_2\n",
      "Done id1_jab_2, 642 frames\n",
      "Start id1_hook_1\n",
      "Done id1_hook_1, 857 frames\n",
      "Start id1_hook_2\n",
      "Done id1_hook_2, 791 frames\n",
      "Start id1_uper_1\n",
      "Done id1_uper_1, 745 frames\n",
      "Start id1_uper_2\n",
      "Done id1_uper_2, 702 frames\n"
     ]
    }
   ],
   "source": [
    "# Load the input image.\n",
    "names = ['id0_jab_1', 'id0_jab_2', \n",
    "         'id0_hook_1', 'id0_hook_2',\n",
    "         'id0_uper_1', 'id0_uper_2',\n",
    "         'id1_jab_1', 'id1_jab_2', \n",
    "         'id1_hook_1', 'id1_hook_2', \n",
    "         'id1_uper_1', 'id1_uper_2']\n",
    "\n",
    "for name in names: \n",
    "    print(f'Start {name}')\n",
    "    \n",
    "    video_path = f'./data/video/{name}.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints = []\n",
    "    i=0\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    image_height, image_width, _ = frame.shape\n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        frame = frame[:,:,::-1]\n",
    "        keypoints_with_scores = run_inference(\n",
    "          movenet, frame, crop_region,\n",
    "          crop_size=[input_size, input_size])\n",
    "        keypoints.append(keypoints_with_scores)\n",
    "        \n",
    "        crop_region = determine_crop_region(\n",
    "          keypoints_with_scores, image_height, image_width)   \n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    np.save(name, np.array(keypoints))\n",
    "    print(f'Done {name}, {i} frames')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S. Check frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = np.load('./data/keypoints/id0_uper_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 1, 1, 17, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start, N = 0, 6\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i in range(N):\n",
    "    dump = draw_prediction_on_image(\n",
    "          np.zeros((300, 300, 3)),\n",
    "          keypoints[start+i], crop_region=None,\n",
    "          close_figure=True, output_image_height=300)\n",
    "    \n",
    "    plt.subplot(1, N, i + 1)\n",
    "    plt.title(f'Frame {i+start}')\n",
    "    plt.imshow(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B57XS0NZPIy",
    "scrolled": false
   },
   "source": [
    "# Prepare gif visualization.\n",
    "output = np.stack(output_images, axis=0)\n",
    "to_gif(output, fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9u_VGR6_BmbZ",
    "5I3xBq80E3N_",
    "L2JmA1xAEntQ"
   ],
   "name": "MoveNet_SinglePose_Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
