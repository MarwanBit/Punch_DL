{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toCy3v03Dwx7"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKe-ubNcDvgv"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqtQzBCpIJ7Y"
   },
   "source": [
    "# MoveNet: Ultra fast and accurate pose detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u_VGR6_BmbZ"
   },
   "source": [
    "## Visualization libraries & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtcwSIcgbIVN"
   },
   "source": [
    "!pip install -q imageio\n",
    "!pip install -q opencv-python\n",
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9BLeJv-pCCld"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "bEJBMeRb3YUy"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, fps):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, fps=fps)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvrN0iQiOxhR"
   },
   "source": [
    "## Load Model from TF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zeGHgANcT7a1"
   },
   "outputs": [],
   "source": [
    "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \n",
    "                    # \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoint_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoint_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKm-B0eMYeg8"
   },
   "source": [
    "## Video keypoints extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdPFXabLyiKv"
   },
   "source": [
    "This section demonstrates how to apply intelligent cropping based on detections from the previous frame when the input is a sequence of frames. This allows the model to devote its attention and resources to the main subject, resulting in much better prediction quality without sacrificing the speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "SYFdK-JHYhrv"
   },
   "outputs": [],
   "source": [
    "#@title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inferece on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2JmA1xAEntQ"
   },
   "source": [
    "### Load videos, extract keypoints and save to *.npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the input image.\n",
    "names = ['id0_jab_1', 'id0_jab_2', \n",
    "         'id0_hook_1', 'id0_hook_2',\n",
    "         'id0_uper_1', 'id0_uper_2',\n",
    "         'id1_jab_1', 'id1_jab_2', \n",
    "         'id1_hook_1', 'id1_hook_2', \n",
    "         'id1_uper_1', 'id1_uper_2']\n",
    "\n",
    "for name in names: \n",
    "    print(f'Start {name}')\n",
    "    \n",
    "    video_path = f'../data/punches/video/{name}.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints = []\n",
    "    i=0\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    image_height, image_width, _ = frame.shape\n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        frame = frame[:,:,::-1]\n",
    "        keypoints_with_scores = run_inference(\n",
    "          movenet, frame, crop_region,\n",
    "          crop_size=[input_size, input_size])\n",
    "        keypoints.append(keypoints_with_scores)\n",
    "        \n",
    "        crop_region = determine_crop_region(\n",
    "          keypoints_with_scores, image_height, image_width)   \n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    np.save(name, np.array(keypoints))\n",
    "    print(f'Done {name}, {i} frames')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S. Check frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = np.load('../data/punches/keypoints/id0_uper_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 1, 1, 17, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start, N = 0, 6\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i in range(N):\n",
    "    dump = draw_prediction_on_image(\n",
    "          np.zeros((300, 300, 3)),\n",
    "          keypoints[start+i], crop_region=None,\n",
    "          close_figure=True, output_image_height=300)\n",
    "    \n",
    "    plt.subplot(1, N, i + 1)\n",
    "    plt.title(f'Frame {i+start}')\n",
    "    plt.imshow(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B57XS0NZPIy",
    "scrolled": false
   },
   "source": [
    "# Prepare gif visualization.\n",
    "output = np.stack(output_images, axis=0)\n",
    "to_gif(output, fps=24)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9u_VGR6_BmbZ",
    "5I3xBq80E3N_",
    "L2JmA1xAEntQ"
   ],
   "name": "MoveNet_SinglePose_Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
