{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mid_points(X):\n",
    "    \"\"\"Calculate middle point between two hips \n",
    "         and substract this point from other coordinates\n",
    "       Returns:\n",
    "         normalized coords with concatenated mid point\n",
    "        \"\"\"\n",
    "    left_hip, right_hip = 11, 12\n",
    "    N = X.shape[0]\n",
    "    mid_points = (X[:, left_hip, :] + X[:, right_hip, :]) / 2\n",
    "    mp = mid_points.reshape(N, 1, 3)\n",
    "    x_n = X - mp\n",
    "    return np.concatenate([x_n, mp], axis=1).reshape(N, 54)\n",
    "\n",
    "\n",
    "def read_data(name, normalize_mp=True):\n",
    "    # print(name)\n",
    "    with open(f'data/labels/{name}') as f:\n",
    "        labels = f.readlines()\n",
    "    \n",
    "    N = int(re.findall(r'\\d+', labels[0])[0])\n",
    "    X = np.load(f'data/keypoints/{name}.npy')\n",
    "    print(X.shape)\n",
    "    X = X.reshape((N, 17, 3))\n",
    "    y = np.zeros(N, dtype=int)\n",
    "    \n",
    "    for lab in labels:\n",
    "        C = re.findall(r'\\d:', lab)\n",
    "\n",
    "        if len(C) == 1:\n",
    "            C = int(C[0][0])\n",
    "            idxs = re.findall(r'\\d+-\\d+', lab)\n",
    "            for idx in idxs:\n",
    "                start, stop = idx.split('-')\n",
    "                y[int(start): int(stop)] = C\n",
    "    if normalize_mp:\n",
    "        X = normalize_mid_points(X)\n",
    "    else:\n",
    "        X = X.reshape(N, 51)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data/labels/')\n",
    "X_list, y_list = [], []\n",
    "X_list_val, y_list_val = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [f for f in files if '_1' in f]\n",
    "val_files = [f for f in files if '_2' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in train_files:\n",
    "    X, y = read_data(f)\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in val_files:\n",
    "    X, y = read_data(f)\n",
    "    X_list_val.append(X)\n",
    "    y_list_val.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(X_list)\n",
    "y_train = np.concatenate(y_list)\n",
    "X_val = np.concatenate(X_list_val)\n",
    "y_val = np.concatenate(y_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "feature_names = []\n",
    "for name in list(KEYPOINT_DICT.keys()):\n",
    "    feature_names.append(name+'-x')\n",
    "    feature_names.append(name+'-y')\n",
    "    feature_names.append(name+'-z')\n",
    "feature_names.append('mid_point-x')\n",
    "feature_names.append('mid_point-y')\n",
    "feature_names.append('mid_point-z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=X_train, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "ax = sns.violinplot(data=df)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=30 # expect camera 30 fps, so process 1 sample per second\n",
    "# Number of samples in batch\n",
    "N_train = X_train.shape[0] // time_steps \n",
    "N_val = X_val.shape[0] // time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:N_train*time_steps].reshape(-1, time_steps, 54)\n",
    "y_train = y_train[:N_train*time_steps].reshape(-1, time_steps, 1)\n",
    "X_val = X_val[:N_val*time_steps].reshape(-1, time_steps, 54)\n",
    "y_val = y_val[:N_val*time_steps].reshape(-1, time_steps, 1)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = K.utils.to_categorical(y_train, num_classes=7)\n",
    "X_train = tf.constant(X_train)\n",
    "y_val = K.utils.to_categorical(y_val, num_classes=7)\n",
    "X_val = tf.constant(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Hyperparams\n",
    "    hp_ru = hp.Int('recurrent_units', min_value=16, max_value=128, step=8)\n",
    "    hp_du1 = hp.Int('units1', min_value=16, max_value=128, step=8)\n",
    "    hp_du2 = hp.Int('units2', min_value=16, max_value=128, step=8)\n",
    "    hp_reg1 = hp.Float('dense1_reg', min_value=1e-4, max_value=1e-2)\n",
    "    hp_reg2 = hp.Float('dense2_reg', min_value=1e-4, max_value=1e-2)\n",
    "    \n",
    "    hp_dp = hp.Float('dropout', min_value=0., max_value=0.4)\n",
    "    hp_rec_dp = hp.Float('rec_dropout', min_value=0., max_value=0.4)\n",
    "    hp_kernel_reg = hp.Float('kernel_reg', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "    hp_recur_reg = hp.Float('recur_reg', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "    hp_act_reg = hp.Float('act_reg', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "    \n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2)\n",
    "\n",
    "    \n",
    "    # Model\n",
    "    norm = tf.keras.layers.LayerNormalization(axis=1)\n",
    "    kernel_reg = K.regularizers.l2(hp_kernel_reg)\n",
    "    recurrent_reg = K.regularizers.l2(hp_recur_reg)\n",
    "    act_reg = K.regularizers.l2(hp_act_reg)\n",
    "    lstm = K.layers.LSTM(hp_ru,\n",
    "                         # stateful=True, # mutable states not supported in tflite \n",
    "                         return_sequences=True,\n",
    "                         kernel_regularizer=kernel_reg,\n",
    "                         recurrent_regularizer=recurrent_reg,\n",
    "                         activity_regularizer=act_reg,\n",
    "                         dropout=hp_dp,\n",
    "                         recurrent_dropout=hp_rec_dp\n",
    "                        )\n",
    "    dense1 = K.layers.Dense(hp_du1, activation='relu', \n",
    "                            kernel_regularizer=K.regularizers.l2(hp_reg1))\n",
    "    dense2 = K.layers.Dense(hp_du2, activation='relu', \n",
    "                           kernel_regularizer=K.regularizers.l2(hp_reg2))\n",
    "    out = K.layers.Dense(7, activation='sigmoid')\n",
    "\n",
    "    model = K.Sequential([norm, lstm, dense1, dense2, out])\n",
    "    model.compile(optimizer=K.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=20,\n",
    "                     directory='kt_dir',\n",
    "                     project_name='punch_dl_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "tuner.search(X_train, y_train, batch_size=1, validation_data=(X_val, y_val), \n",
    "             epochs=32, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 64 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, batch_size=1, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=64,\n",
    "                   callbacks=[K.callbacks.ReduceLROnPlateau()])\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(221)\n",
    "plt.title(\"Train Loss\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.subplot(222)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "We need more data to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model) \n",
    "# converter.experimental_enable_resource_variables = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with TensorFlow to get expected results.\n",
    "TEST_CASES = 10\n",
    "x = tf.reshape(X_val, (-1, time_steps, 54))\n",
    "# Run the model with TensorFlow Lite\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "for i in range(TEST_CASES):\n",
    "    expected = model.predict(x[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], x[i:i+1])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Assert if the result of TFLite model is consistent with the TF model.\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "    # Please note: TfLite fused Lstm kernel is stateful, so we need to reset\n",
    "    # the states.\n",
    "    # Clean up internal states.\n",
    "    interpreter.reset_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving most frequent smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model(x).numpy(), axis=-1).ravel()\n",
    "y_val_np = np.concatenate(y_list_val)[:N_val*time_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1,1,1,1,3,3,1,1] -> [1]\n",
    "# moving window across padded y_pred\n",
    "# output element is most frequent in window\n",
    "\n",
    "win_size = time_steps // 2\n",
    "mf_smooth = np.zeros(y_pred.shape[0]+win_size)\n",
    "for i in range(len(y_pred) - win_size):\n",
    "    window = y_pred[i:i+win_size]\n",
    "    bins = np.bincount(window)\n",
    "    mf_smooth[i+win_size//2] = np.argmax(bins)\n",
    "\n",
    "mf_smooth = mf_smooth[:-win_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mf_smooth), len(y_pred), len(y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_val_np==y_pred))\n",
    "print(np.mean(y_val_np==mf_smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original')\n",
    "plt.plot(y_val_np[:600])\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Prediction')\n",
    "plt.plot(y_pred[:600])\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Smoothed prediction')\n",
    "plt.plot(mf_smooth[:600])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to:  \n",
    "https://github.com/keras-team/keras-tuner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
